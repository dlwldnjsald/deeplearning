{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 8s 758ms/step - loss: 0.3795 - accuracy: 0.7714 - val_loss: 0.3545 - val_accuracy: 0.8000\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3393 - accuracy: 0.8368 - val_loss: 0.3221 - val_accuracy: 0.8762\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.3108 - accuracy: 0.8714 - val_loss: 0.2922 - val_accuracy: 0.8992\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2847 - accuracy: 0.9045 - val_loss: 0.2701 - val_accuracy: 0.9185\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2638 - accuracy: 0.9151 - val_loss: 0.2550 - val_accuracy: 0.9292\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2504 - accuracy: 0.9258 - val_loss: 0.2443 - val_accuracy: 0.9346\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2403 - accuracy: 0.9258 - val_loss: 0.2354 - val_accuracy: 0.9331\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2296 - accuracy: 0.9294 - val_loss: 0.2247 - val_accuracy: 0.9331\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2207 - accuracy: 0.9269 - val_loss: 0.2169 - val_accuracy: 0.9315\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2172 - accuracy: 0.9256 - val_loss: 0.2138 - val_accuracy: 0.9308\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2149 - accuracy: 0.9256 - val_loss: 0.2099 - val_accuracy: 0.9315\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2107 - accuracy: 0.9292 - val_loss: 0.2071 - val_accuracy: 0.9354\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2100 - accuracy: 0.9253 - val_loss: 0.2081 - val_accuracy: 0.9362\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2105 - accuracy: 0.9238 - val_loss: 0.2010 - val_accuracy: 0.9369\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2056 - accuracy: 0.9276 - val_loss: 0.1982 - val_accuracy: 0.9338\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2035 - accuracy: 0.9299 - val_loss: 0.1970 - val_accuracy: 0.9362\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2011 - accuracy: 0.9302 - val_loss: 0.1952 - val_accuracy: 0.9369\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1997 - accuracy: 0.9315 - val_loss: 0.1922 - val_accuracy: 0.9346\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1974 - accuracy: 0.9310 - val_loss: 0.1905 - val_accuracy: 0.9354\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1982 - accuracy: 0.9315 - val_loss: 0.1909 - val_accuracy: 0.9362\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1981 - accuracy: 0.9305 - val_loss: 0.1872 - val_accuracy: 0.9362\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1959 - accuracy: 0.9323 - val_loss: 0.1874 - val_accuracy: 0.9385\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1929 - accuracy: 0.9307 - val_loss: 0.1843 - val_accuracy: 0.9400\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1910 - accuracy: 0.9328 - val_loss: 0.1842 - val_accuracy: 0.9369\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1899 - accuracy: 0.9335 - val_loss: 0.1813 - val_accuracy: 0.9400\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1875 - accuracy: 0.9338 - val_loss: 0.1800 - val_accuracy: 0.9400\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1866 - accuracy: 0.9341 - val_loss: 0.1791 - val_accuracy: 0.9408\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1854 - accuracy: 0.9346 - val_loss: 0.1770 - val_accuracy: 0.9400\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1855 - accuracy: 0.9351 - val_loss: 0.1789 - val_accuracy: 0.9369\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1860 - accuracy: 0.9374 - val_loss: 0.1763 - val_accuracy: 0.9431\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1821 - accuracy: 0.9361 - val_loss: 0.1728 - val_accuracy: 0.9438\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1798 - accuracy: 0.9371 - val_loss: 0.1729 - val_accuracy: 0.9400\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1802 - accuracy: 0.9384 - val_loss: 0.1726 - val_accuracy: 0.9446\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1776 - accuracy: 0.9387 - val_loss: 0.1683 - val_accuracy: 0.9431\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1758 - accuracy: 0.9397 - val_loss: 0.1671 - val_accuracy: 0.9438\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1743 - accuracy: 0.9394 - val_loss: 0.1666 - val_accuracy: 0.9469\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1735 - accuracy: 0.9389 - val_loss: 0.1639 - val_accuracy: 0.9446\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1724 - accuracy: 0.9392 - val_loss: 0.1652 - val_accuracy: 0.9423\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1745 - accuracy: 0.9382 - val_loss: 0.1684 - val_accuracy: 0.9400\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1696 - accuracy: 0.9415 - val_loss: 0.1587 - val_accuracy: 0.9485\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1669 - accuracy: 0.9425 - val_loss: 0.1573 - val_accuracy: 0.9492\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1654 - accuracy: 0.9428 - val_loss: 0.1555 - val_accuracy: 0.9492\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1644 - accuracy: 0.9433 - val_loss: 0.1539 - val_accuracy: 0.9485\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1630 - accuracy: 0.9430 - val_loss: 0.1530 - val_accuracy: 0.9492\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1616 - accuracy: 0.9438 - val_loss: 0.1518 - val_accuracy: 0.9485\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1599 - accuracy: 0.9435 - val_loss: 0.1489 - val_accuracy: 0.9477\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1569 - accuracy: 0.9438 - val_loss: 0.1482 - val_accuracy: 0.9462\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1538 - accuracy: 0.9459 - val_loss: 0.1438 - val_accuracy: 0.9485\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1507 - accuracy: 0.9471 - val_loss: 0.1406 - val_accuracy: 0.9515\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1484 - accuracy: 0.9479 - val_loss: 0.1458 - val_accuracy: 0.9492\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9538\n",
      "Test accuracy: 0.9538461565971375\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.7485.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.7985.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.7415.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.8723.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.9000.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.9185.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9369.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9392.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9385.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9385.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9408.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9400.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9408.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9408.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9400.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9431.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9431.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9408.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9423.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9415.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9415.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9415.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9415.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9415.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9423.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9423.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9423.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9423.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9423.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9423.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9423.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9431.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9423.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9431.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9446.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9446.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9446.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9446.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9454.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9454.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9446.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9454.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9454.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9462.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9446.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9469.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9446.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9446.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9492.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9454.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9369\n",
      "Test accuracy: 0.936923086643219\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.160605</td>\n",
       "      <td>0.941750</td>\n",
       "      <td>0.149567</td>\n",
       "      <td>0.946923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.159685</td>\n",
       "      <td>0.940980</td>\n",
       "      <td>0.148557</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.158731</td>\n",
       "      <td>0.943290</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157193</td>\n",
       "      <td>0.942520</td>\n",
       "      <td>0.147482</td>\n",
       "      <td>0.946154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156131</td>\n",
       "      <td>0.943290</td>\n",
       "      <td>0.146733</td>\n",
       "      <td>0.946923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.994355</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>0.990769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.021344</td>\n",
       "      <td>0.994611</td>\n",
       "      <td>0.034986</td>\n",
       "      <td>0.992308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.022155</td>\n",
       "      <td>0.992815</td>\n",
       "      <td>0.041280</td>\n",
       "      <td>0.989231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.022104</td>\n",
       "      <td>0.992815</td>\n",
       "      <td>0.042433</td>\n",
       "      <td>0.988462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.022450</td>\n",
       "      <td>0.992815</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.160605  0.941750  0.149567      0.946923\n",
       "1     0.159685  0.940980  0.148557      0.946154\n",
       "2     0.158731  0.943290  0.148315      0.946154\n",
       "3     0.157193  0.942520  0.147482      0.946154\n",
       "4     0.156131  0.943290  0.146733      0.946923\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.019737  0.994355  0.037509      0.990769\n",
       "1996  0.021344  0.994611  0.034986      0.992308\n",
       "1997  0.022155  0.992815  0.041280      0.989231\n",
       "1998  0.022104  0.992815  0.042433      0.988462\n",
       "1999  0.022450  0.992815  0.044943      0.985385\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 31ms/step - loss: 0.9680 - accuracy: 0.7601 - val_loss: 0.7060 - val_accuracy: 0.7385\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.5012 - accuracy: 0.7606 - val_loss: 0.4500 - val_accuracy: 0.7392\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4296 - accuracy: 0.7701 - val_loss: 0.4257 - val_accuracy: 0.7715\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3795 - accuracy: 0.7888 - val_loss: 0.4010 - val_accuracy: 0.7708\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3620 - accuracy: 0.8063 - val_loss: 0.3773 - val_accuracy: 0.8100\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3349 - accuracy: 0.8478 - val_loss: 0.3441 - val_accuracy: 0.8662\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3127 - accuracy: 0.8848 - val_loss: 0.3205 - val_accuracy: 0.8977\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.2902 - accuracy: 0.9025 - val_loss: 0.2997 - val_accuracy: 0.9038\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2729 - accuracy: 0.9066 - val_loss: 0.2790 - val_accuracy: 0.9023\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2598 - accuracy: 0.9069 - val_loss: 0.2658 - val_accuracy: 0.9023\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2509 - accuracy: 0.9079 - val_loss: 0.2573 - val_accuracy: 0.9046\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2426 - accuracy: 0.9102 - val_loss: 0.2462 - val_accuracy: 0.9015\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2352 - accuracy: 0.9130 - val_loss: 0.2369 - val_accuracy: 0.9085\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2282 - accuracy: 0.9207 - val_loss: 0.2324 - val_accuracy: 0.9123\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2229 - accuracy: 0.9233 - val_loss: 0.2231 - val_accuracy: 0.9146\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2155 - accuracy: 0.9274 - val_loss: 0.2186 - val_accuracy: 0.9162\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.2106 - accuracy: 0.9289 - val_loss: 0.2122 - val_accuracy: 0.9162\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2053 - accuracy: 0.9338 - val_loss: 0.2069 - val_accuracy: 0.9177\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2012 - accuracy: 0.9335 - val_loss: 0.2018 - val_accuracy: 0.9208\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1968 - accuracy: 0.9384 - val_loss: 0.1997 - val_accuracy: 0.9254\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1920 - accuracy: 0.9369 - val_loss: 0.1921 - val_accuracy: 0.9277\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1895 - accuracy: 0.9379 - val_loss: 0.1890 - val_accuracy: 0.9285\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1845 - accuracy: 0.9394 - val_loss: 0.1840 - val_accuracy: 0.9285\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1821 - accuracy: 0.9382 - val_loss: 0.1791 - val_accuracy: 0.9338\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1777 - accuracy: 0.9387 - val_loss: 0.1808 - val_accuracy: 0.9285\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1731 - accuracy: 0.9400 - val_loss: 0.1702 - val_accuracy: 0.9362\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1681 - accuracy: 0.9423 - val_loss: 0.1684 - val_accuracy: 0.9331\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1645 - accuracy: 0.9438 - val_loss: 0.1641 - val_accuracy: 0.9354\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1620 - accuracy: 0.9430 - val_loss: 0.1595 - val_accuracy: 0.9392\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1593 - accuracy: 0.9456 - val_loss: 0.1566 - val_accuracy: 0.9385\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1554 - accuracy: 0.9471 - val_loss: 0.1580 - val_accuracy: 0.9385\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1542 - accuracy: 0.9487 - val_loss: 0.1507 - val_accuracy: 0.9408\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1509 - accuracy: 0.9469 - val_loss: 0.1478 - val_accuracy: 0.9423\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1481 - accuracy: 0.9492 - val_loss: 0.1454 - val_accuracy: 0.9423\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1457 - accuracy: 0.9494 - val_loss: 0.1440 - val_accuracy: 0.9423\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1431 - accuracy: 0.9510 - val_loss: 0.1453 - val_accuracy: 0.9415\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1400 - accuracy: 0.9520 - val_loss: 0.1388 - val_accuracy: 0.9438\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1382 - accuracy: 0.9520 - val_loss: 0.1394 - val_accuracy: 0.9431\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1355 - accuracy: 0.9523 - val_loss: 0.1341 - val_accuracy: 0.9454\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1333 - accuracy: 0.9548 - val_loss: 0.1316 - val_accuracy: 0.9462\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1321 - accuracy: 0.9528 - val_loss: 0.1442 - val_accuracy: 0.9423\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1336 - accuracy: 0.9543 - val_loss: 0.1382 - val_accuracy: 0.9438\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1296 - accuracy: 0.9530 - val_loss: 0.1257 - val_accuracy: 0.9492\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1275 - accuracy: 0.9538 - val_loss: 0.1238 - val_accuracy: 0.9554\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1262 - accuracy: 0.9566 - val_loss: 0.1387 - val_accuracy: 0.9446\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1282 - accuracy: 0.9556 - val_loss: 0.1305 - val_accuracy: 0.9462\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9536 - val_loss: 0.1255 - val_accuracy: 0.9469\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1220 - accuracy: 0.9559 - val_loss: 0.1171 - val_accuracy: 0.9531\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1193 - accuracy: 0.9561 - val_loss: 0.1178 - val_accuracy: 0.9600\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1179 - accuracy: 0.9571 - val_loss: 0.1152 - val_accuracy: 0.9546\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1152 - accuracy: 0.9582 - val_loss: 0.1135 - val_accuracy: 0.9554\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1135 - accuracy: 0.9592 - val_loss: 0.1129 - val_accuracy: 0.9569\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1138 - accuracy: 0.9610 - val_loss: 0.1132 - val_accuracy: 0.9538\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1131 - accuracy: 0.9602 - val_loss: 0.1073 - val_accuracy: 0.9662\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1098 - accuracy: 0.9620 - val_loss: 0.1072 - val_accuracy: 0.9677\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1105 - accuracy: 0.9615 - val_loss: 0.1045 - val_accuracy: 0.9646\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1086 - accuracy: 0.9610 - val_loss: 0.1030 - val_accuracy: 0.9692\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1080 - accuracy: 0.9636 - val_loss: 0.1021 - val_accuracy: 0.9692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1054 - accuracy: 0.9654 - val_loss: 0.1037 - val_accuracy: 0.9654\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1042 - accuracy: 0.9659 - val_loss: 0.0997 - val_accuracy: 0.9692\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1044 - accuracy: 0.9646 - val_loss: 0.1004 - val_accuracy: 0.9677\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1022 - accuracy: 0.9656 - val_loss: 0.1002 - val_accuracy: 0.9677\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1021 - accuracy: 0.9656 - val_loss: 0.0972 - val_accuracy: 0.9700\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1010 - accuracy: 0.9692 - val_loss: 0.0960 - val_accuracy: 0.9723\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1018 - accuracy: 0.9656 - val_loss: 0.0968 - val_accuracy: 0.9746\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0987 - accuracy: 0.9687 - val_loss: 0.0955 - val_accuracy: 0.9700\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0980 - accuracy: 0.9666 - val_loss: 0.0947 - val_accuracy: 0.9754\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0986 - accuracy: 0.9654 - val_loss: 0.0942 - val_accuracy: 0.9769\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0981 - accuracy: 0.9664 - val_loss: 0.0930 - val_accuracy: 0.9738\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0972 - accuracy: 0.9674 - val_loss: 0.0924 - val_accuracy: 0.9762\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0961 - accuracy: 0.9682 - val_loss: 0.0919 - val_accuracy: 0.9762\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0961 - accuracy: 0.9682 - val_loss: 0.0932 - val_accuracy: 0.9692\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0944 - accuracy: 0.9695 - val_loss: 0.0904 - val_accuracy: 0.9731\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0934 - accuracy: 0.9700 - val_loss: 0.0898 - val_accuracy: 0.9715\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0931 - accuracy: 0.9707 - val_loss: 0.0898 - val_accuracy: 0.9731\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0927 - accuracy: 0.9710 - val_loss: 0.0886 - val_accuracy: 0.9777\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0912 - accuracy: 0.9718 - val_loss: 0.0896 - val_accuracy: 0.9715\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0919 - accuracy: 0.9715 - val_loss: 0.0925 - val_accuracy: 0.9692\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0915 - accuracy: 0.9713 - val_loss: 0.0927 - val_accuracy: 0.9692\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0945 - accuracy: 0.9702 - val_loss: 0.0867 - val_accuracy: 0.9731\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0908 - accuracy: 0.9710 - val_loss: 0.0849 - val_accuracy: 0.9769\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0926 - accuracy: 0.9692 - val_loss: 0.0862 - val_accuracy: 0.9731\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0907 - accuracy: 0.9697 - val_loss: 0.0850 - val_accuracy: 0.9754\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0881 - accuracy: 0.9728 - val_loss: 0.0839 - val_accuracy: 0.9754\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0866 - accuracy: 0.9723 - val_loss: 0.0858 - val_accuracy: 0.9715\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0867 - accuracy: 0.9728 - val_loss: 0.0861 - val_accuracy: 0.9715\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0879 - accuracy: 0.9715 - val_loss: 0.0857 - val_accuracy: 0.9731\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0888 - accuracy: 0.9723 - val_loss: 0.0818 - val_accuracy: 0.9777\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0845 - accuracy: 0.9741 - val_loss: 0.0812 - val_accuracy: 0.9762\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0860 - accuracy: 0.9741 - val_loss: 0.0815 - val_accuracy: 0.9769\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0852 - accuracy: 0.9728 - val_loss: 0.0841 - val_accuracy: 0.9731\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0840 - accuracy: 0.9725 - val_loss: 0.0815 - val_accuracy: 0.9746\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0835 - accuracy: 0.9738 - val_loss: 0.0795 - val_accuracy: 0.9785\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0833 - accuracy: 0.9718 - val_loss: 0.0791 - val_accuracy: 0.9792\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0823 - accuracy: 0.9749 - val_loss: 0.0788 - val_accuracy: 0.9800\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0834 - accuracy: 0.9738 - val_loss: 0.0823 - val_accuracy: 0.9731\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0851 - accuracy: 0.9743 - val_loss: 0.0797 - val_accuracy: 0.9762\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0836 - accuracy: 0.9728 - val_loss: 0.0831 - val_accuracy: 0.9731\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0823 - accuracy: 0.9751 - val_loss: 0.0944 - val_accuracy: 0.9669\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0856 - accuracy: 0.9720 - val_loss: 0.0847 - val_accuracy: 0.9723\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0817 - accuracy: 0.9749 - val_loss: 0.0803 - val_accuracy: 0.9746\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0808 - accuracy: 0.9746 - val_loss: 0.0786 - val_accuracy: 0.9754\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0791 - accuracy: 0.9738 - val_loss: 0.0777 - val_accuracy: 0.9762\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0783 - accuracy: 0.9761 - val_loss: 0.0761 - val_accuracy: 0.9792\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0802 - accuracy: 0.9746 - val_loss: 0.0768 - val_accuracy: 0.9792\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0788 - accuracy: 0.9738 - val_loss: 0.0761 - val_accuracy: 0.9792\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0808 - accuracy: 0.9751 - val_loss: 0.0742 - val_accuracy: 0.9792\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0771 - accuracy: 0.9751 - val_loss: 0.0788 - val_accuracy: 0.9746\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0778 - accuracy: 0.9754 - val_loss: 0.0797 - val_accuracy: 0.9738\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0770 - accuracy: 0.9746 - val_loss: 0.0733 - val_accuracy: 0.9792\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0765 - accuracy: 0.9754 - val_loss: 0.0731 - val_accuracy: 0.9800\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0776 - accuracy: 0.9751 - val_loss: 0.0725 - val_accuracy: 0.9792\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0755 - accuracy: 0.9764 - val_loss: 0.0737 - val_accuracy: 0.9785\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0761 - accuracy: 0.9746 - val_loss: 0.0728 - val_accuracy: 0.9808\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0753 - accuracy: 0.9766 - val_loss: 0.0723 - val_accuracy: 0.9792\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0776 - accuracy: 0.9751 - val_loss: 0.0712 - val_accuracy: 0.9785\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0752 - accuracy: 0.9761 - val_loss: 0.0733 - val_accuracy: 0.9777\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0745 - accuracy: 0.9761 - val_loss: 0.0835 - val_accuracy: 0.9723\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0755 - accuracy: 0.9743 - val_loss: 0.0756 - val_accuracy: 0.9769\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0791 - accuracy: 0.9749 - val_loss: 0.0710 - val_accuracy: 0.9792\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0717 - accuracy: 0.9772 - val_loss: 0.0701 - val_accuracy: 0.9785\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0725 - accuracy: 0.9774 - val_loss: 0.0700 - val_accuracy: 0.9792\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0723 - accuracy: 0.9772 - val_loss: 0.0695 - val_accuracy: 0.9792\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0724 - accuracy: 0.9772 - val_loss: 0.0748 - val_accuracy: 0.9754\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0721 - accuracy: 0.9772 - val_loss: 0.0718 - val_accuracy: 0.9769\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0738 - accuracy: 0.9777 - val_loss: 0.0815 - val_accuracy: 0.9723\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0736 - accuracy: 0.9769 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0714 - accuracy: 0.9777 - val_loss: 0.0675 - val_accuracy: 0.9785\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0691 - accuracy: 0.9787 - val_loss: 0.0674 - val_accuracy: 0.9777\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0691 - accuracy: 0.9777 - val_loss: 0.0672 - val_accuracy: 0.9785\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0689 - accuracy: 0.9800 - val_loss: 0.0726 - val_accuracy: 0.9746\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0698 - accuracy: 0.9792 - val_loss: 0.0675 - val_accuracy: 0.9785\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0695 - accuracy: 0.9782 - val_loss: 0.0658 - val_accuracy: 0.9792\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0791 - accuracy: 0.9731 - val_loss: 0.0655 - val_accuracy: 0.9800\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0723 - accuracy: 0.9777 - val_loss: 0.0713 - val_accuracy: 0.9808\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0708 - accuracy: 0.9779 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0696 - accuracy: 0.9784 - val_loss: 0.0647 - val_accuracy: 0.9815\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0682 - accuracy: 0.9787 - val_loss: 0.0648 - val_accuracy: 0.9800\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0675 - accuracy: 0.9784 - val_loss: 0.0639 - val_accuracy: 0.9823\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0681 - accuracy: 0.9813 - val_loss: 0.0660 - val_accuracy: 0.9754\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0684 - accuracy: 0.9787 - val_loss: 0.0635 - val_accuracy: 0.9808\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0672 - accuracy: 0.9784 - val_loss: 0.0642 - val_accuracy: 0.9808\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0690 - accuracy: 0.9826 - val_loss: 0.0630 - val_accuracy: 0.9823\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0674 - accuracy: 0.9810 - val_loss: 0.0627 - val_accuracy: 0.9808\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0652 - accuracy: 0.9808 - val_loss: 0.0624 - val_accuracy: 0.9815\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0649 - accuracy: 0.9805 - val_loss: 0.0622 - val_accuracy: 0.9815\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0649 - accuracy: 0.9805 - val_loss: 0.0624 - val_accuracy: 0.9815\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9810 - val_loss: 0.0650 - val_accuracy: 0.9823\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0666 - accuracy: 0.9810 - val_loss: 0.0627 - val_accuracy: 0.9808\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0657 - accuracy: 0.9784 - val_loss: 0.0660 - val_accuracy: 0.9815\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0683 - accuracy: 0.9797 - val_loss: 0.0655 - val_accuracy: 0.9823\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0700 - accuracy: 0.9805 - val_loss: 0.0625 - val_accuracy: 0.9808\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0657 - accuracy: 0.9800 - val_loss: 0.0614 - val_accuracy: 0.9792\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.9787 - val_loss: 0.0620 - val_accuracy: 0.9815\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0646 - accuracy: 0.9818 - val_loss: 0.0663 - val_accuracy: 0.9762\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0638 - accuracy: 0.9797 - val_loss: 0.0620 - val_accuracy: 0.9815\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0632 - accuracy: 0.9808 - val_loss: 0.0598 - val_accuracy: 0.9838\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0634 - accuracy: 0.9813 - val_loss: 0.0598 - val_accuracy: 0.9838\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0625 - accuracy: 0.9810 - val_loss: 0.0597 - val_accuracy: 0.9823\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0634 - accuracy: 0.9820 - val_loss: 0.0624 - val_accuracy: 0.9823\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0641 - accuracy: 0.9808 - val_loss: 0.0610 - val_accuracy: 0.9823\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0644 - accuracy: 0.9813 - val_loss: 0.0594 - val_accuracy: 0.9815\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0650 - accuracy: 0.9802 - val_loss: 0.0695 - val_accuracy: 0.9762\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0690 - accuracy: 0.9802 - val_loss: 0.0713 - val_accuracy: 0.9754\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0674 - accuracy: 0.9808 - val_loss: 0.0730 - val_accuracy: 0.9754\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0664 - accuracy: 0.9810 - val_loss: 0.0643 - val_accuracy: 0.9777\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0645 - accuracy: 0.9805 - val_loss: 0.0617 - val_accuracy: 0.9800\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0632 - accuracy: 0.9805 - val_loss: 0.0582 - val_accuracy: 0.9838\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0606 - accuracy: 0.9815 - val_loss: 0.0593 - val_accuracy: 0.9815\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0613 - accuracy: 0.9813 - val_loss: 0.0613 - val_accuracy: 0.9792\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0621 - accuracy: 0.9810 - val_loss: 0.0710 - val_accuracy: 0.9754\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0654 - accuracy: 0.9813 - val_loss: 0.0643 - val_accuracy: 0.9769\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0675 - accuracy: 0.9805 - val_loss: 0.0640 - val_accuracy: 0.9785\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 0.0575 - val_accuracy: 0.9823\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0626 - accuracy: 0.9813 - val_loss: 0.0593 - val_accuracy: 0.9823\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0653 - accuracy: 0.9815 - val_loss: 0.0577 - val_accuracy: 0.9831\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0634 - accuracy: 0.9795 - val_loss: 0.0596 - val_accuracy: 0.9838\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0612 - accuracy: 0.9815 - val_loss: 0.0573 - val_accuracy: 0.9838\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0574 - val_accuracy: 0.9838\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0572 - val_accuracy: 0.9846\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0613 - accuracy: 0.9813 - val_loss: 0.0585 - val_accuracy: 0.9846\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0594 - accuracy: 0.9826 - val_loss: 0.0578 - val_accuracy: 0.9823\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0587 - accuracy: 0.9826 - val_loss: 0.0637 - val_accuracy: 0.9785\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0624 - accuracy: 0.9813 - val_loss: 0.0583 - val_accuracy: 0.9823\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0595 - accuracy: 0.9828 - val_loss: 0.0572 - val_accuracy: 0.9823\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0557 - val_accuracy: 0.9846\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0587 - accuracy: 0.9813 - val_loss: 0.0559 - val_accuracy: 0.9838\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9826 - val_loss: 0.0583 - val_accuracy: 0.9838\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0562 - val_accuracy: 0.9846\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0580 - accuracy: 0.9841 - val_loss: 0.0554 - val_accuracy: 0.9838\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0581 - accuracy: 0.9818 - val_loss: 0.0555 - val_accuracy: 0.9838\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0574 - accuracy: 0.9828 - val_loss: 0.0549 - val_accuracy: 0.9846\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0576 - accuracy: 0.9813 - val_loss: 0.0549 - val_accuracy: 0.9838\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0569 - accuracy: 0.9826 - val_loss: 0.0548 - val_accuracy: 0.9846\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0567 - accuracy: 0.9826 - val_loss: 0.0555 - val_accuracy: 0.9831\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0577 - accuracy: 0.9828 - val_loss: 0.0561 - val_accuracy: 0.9815\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9810 - val_loss: 0.0586 - val_accuracy: 0.9815\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9810 - val_loss: 0.0691 - val_accuracy: 0.9777\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0635 - accuracy: 0.9808 - val_loss: 0.0711 - val_accuracy: 0.9769\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0663 - accuracy: 0.9800 - val_loss: 0.0543 - val_accuracy: 0.9838\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0582 - accuracy: 0.9823 - val_loss: 0.0553 - val_accuracy: 0.9846\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 0.9846\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0570 - accuracy: 0.9831 - val_loss: 0.0543 - val_accuracy: 0.9831\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0568 - accuracy: 0.9828 - val_loss: 0.0535 - val_accuracy: 0.9846\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.0533 - val_accuracy: 0.9846\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0555 - accuracy: 0.9838 - val_loss: 0.0537 - val_accuracy: 0.9838\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0574 - accuracy: 0.9838 - val_loss: 0.0568 - val_accuracy: 0.9838\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0587 - accuracy: 0.9823 - val_loss: 0.0531 - val_accuracy: 0.9854\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.0532 - val_accuracy: 0.9846\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.0556 - val_accuracy: 0.9838\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.0532 - val_accuracy: 0.9838\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0555 - accuracy: 0.9843 - val_loss: 0.0533 - val_accuracy: 0.9846\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9820 - val_loss: 0.0556 - val_accuracy: 0.9838\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0558 - accuracy: 0.9823 - val_loss: 0.0538 - val_accuracy: 0.9854\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9818 - val_loss: 0.0532 - val_accuracy: 0.9831\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.0548 - val_accuracy: 0.9831\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0562 - accuracy: 0.9831 - val_loss: 0.0588 - val_accuracy: 0.9823\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0590 - accuracy: 0.9826 - val_loss: 0.0619 - val_accuracy: 0.9800\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0573 - accuracy: 0.9841 - val_loss: 0.0571 - val_accuracy: 0.9823\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0593 - accuracy: 0.9820 - val_loss: 0.0523 - val_accuracy: 0.9831\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0599 - accuracy: 0.9805 - val_loss: 0.0559 - val_accuracy: 0.9838\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0563 - accuracy: 0.9831 - val_loss: 0.0553 - val_accuracy: 0.9854\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0577 - accuracy: 0.9820 - val_loss: 0.0540 - val_accuracy: 0.9854\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 0.0541 - val_accuracy: 0.9854\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0607 - accuracy: 0.9818 - val_loss: 0.0519 - val_accuracy: 0.9846\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0551 - accuracy: 0.9836 - val_loss: 0.0540 - val_accuracy: 0.9831\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0576 - accuracy: 0.9841 - val_loss: 0.0649 - val_accuracy: 0.9785\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0581 - accuracy: 0.9818 - val_loss: 0.0545 - val_accuracy: 0.9838\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0556 - accuracy: 0.9836 - val_loss: 0.0629 - val_accuracy: 0.9800\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.0522 - val_accuracy: 0.9838\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0518 - val_accuracy: 0.9846\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9828 - val_loss: 0.0523 - val_accuracy: 0.9846\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0532 - accuracy: 0.9833 - val_loss: 0.0533 - val_accuracy: 0.9854\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0547 - accuracy: 0.9841 - val_loss: 0.0520 - val_accuracy: 0.9846\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0570 - accuracy: 0.9831 - val_loss: 0.0608 - val_accuracy: 0.9808\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.0658 - val_accuracy: 0.9769\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0539 - accuracy: 0.9849 - val_loss: 0.0511 - val_accuracy: 0.9838\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0512 - val_accuracy: 0.9831\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 0.9841 - val_loss: 0.0516 - val_accuracy: 0.9854\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0529 - accuracy: 0.9843 - val_loss: 0.0509 - val_accuracy: 0.9862\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0516 - val_accuracy: 0.9862\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9836 - val_loss: 0.0538 - val_accuracy: 0.9846\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9846 - val_loss: 0.0523 - val_accuracy: 0.9846\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9826 - val_loss: 0.0517 - val_accuracy: 0.9846\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0523 - accuracy: 0.9849 - val_loss: 0.0508 - val_accuracy: 0.9846\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0531 - accuracy: 0.9838 - val_loss: 0.0521 - val_accuracy: 0.9838\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9841 - val_loss: 0.0518 - val_accuracy: 0.9846\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0529 - accuracy: 0.9836 - val_loss: 0.0504 - val_accuracy: 0.9854\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0517 - accuracy: 0.9849 - val_loss: 0.0538 - val_accuracy: 0.9846\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.0507 - val_accuracy: 0.9862\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0530 - accuracy: 0.9836 - val_loss: 0.0499 - val_accuracy: 0.9854\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0518 - accuracy: 0.9838 - val_loss: 0.0513 - val_accuracy: 0.9854\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0540 - accuracy: 0.9843 - val_loss: 0.0508 - val_accuracy: 0.9838\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0520 - accuracy: 0.9833 - val_loss: 0.0502 - val_accuracy: 0.9862\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0525 - accuracy: 0.9846 - val_loss: 0.0508 - val_accuracy: 0.9846\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0520 - accuracy: 0.9836 - val_loss: 0.0502 - val_accuracy: 0.9838\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0511 - accuracy: 0.9846 - val_loss: 0.0509 - val_accuracy: 0.9846\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9836 - val_loss: 0.0572 - val_accuracy: 0.9831\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9831 - val_loss: 0.0505 - val_accuracy: 0.9854\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0554 - accuracy: 0.9836 - val_loss: 0.0494 - val_accuracy: 0.9854\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0550 - accuracy: 0.9836 - val_loss: 0.0550 - val_accuracy: 0.9862\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.0515 - val_accuracy: 0.9862\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0527 - accuracy: 0.9833 - val_loss: 0.0509 - val_accuracy: 0.9862\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0521 - accuracy: 0.9838 - val_loss: 0.0494 - val_accuracy: 0.9869\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9833 - val_loss: 0.0515 - val_accuracy: 0.9862\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0515 - accuracy: 0.9846 - val_loss: 0.0490 - val_accuracy: 0.9862\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0518 - accuracy: 0.9838 - val_loss: 0.0488 - val_accuracy: 0.9877\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9843 - val_loss: 0.0493 - val_accuracy: 0.9846\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9831 - val_loss: 0.0490 - val_accuracy: 0.9854\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9843 - val_loss: 0.0516 - val_accuracy: 0.9846\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9851 - val_loss: 0.0491 - val_accuracy: 0.9854\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.0490 - val_accuracy: 0.9869\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0507 - accuracy: 0.9836 - val_loss: 0.0501 - val_accuracy: 0.9854\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9856 - val_loss: 0.0621 - val_accuracy: 0.9785\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0570 - accuracy: 0.9828 - val_loss: 0.0569 - val_accuracy: 0.9823\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0535 - accuracy: 0.9820 - val_loss: 0.0517 - val_accuracy: 0.9846\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9843 - val_loss: 0.0564 - val_accuracy: 0.9823\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0521 - accuracy: 0.9838 - val_loss: 0.0489 - val_accuracy: 0.9869\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0502 - accuracy: 0.9849 - val_loss: 0.0485 - val_accuracy: 0.9877\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9838 - val_loss: 0.0522 - val_accuracy: 0.9854\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 0.9818 - val_loss: 0.0495 - val_accuracy: 0.9862\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9849 - val_loss: 0.0497 - val_accuracy: 0.9854\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0514 - accuracy: 0.9836 - val_loss: 0.0489 - val_accuracy: 0.9869\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 0.9851 - val_loss: 0.0507 - val_accuracy: 0.9862\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0513 - accuracy: 0.9846 - val_loss: 0.0531 - val_accuracy: 0.9846\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.9841 - val_loss: 0.0534 - val_accuracy: 0.9846\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9854 - val_loss: 0.0530 - val_accuracy: 0.9846\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0519 - accuracy: 0.9838 - val_loss: 0.0482 - val_accuracy: 0.9877\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.0490 - val_accuracy: 0.9854\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0504 - accuracy: 0.9833 - val_loss: 0.0485 - val_accuracy: 0.9862\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0497 - accuracy: 0.9854 - val_loss: 0.0501 - val_accuracy: 0.9854\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0530 - accuracy: 0.9836 - val_loss: 0.0529 - val_accuracy: 0.9854\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9810 - val_loss: 0.0519 - val_accuracy: 0.9854\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.0493 - val_accuracy: 0.9854\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9849 - val_loss: 0.0495 - val_accuracy: 0.9862\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0487 - accuracy: 0.9851 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9841 - val_loss: 0.0490 - val_accuracy: 0.9862\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.0487 - val_accuracy: 0.9869\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0493 - accuracy: 0.9864 - val_loss: 0.0485 - val_accuracy: 0.9877\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0487 - accuracy: 0.9849 - val_loss: 0.0513 - val_accuracy: 0.9846\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0490 - accuracy: 0.9854 - val_loss: 0.0492 - val_accuracy: 0.9862\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0496 - accuracy: 0.9854 - val_loss: 0.0485 - val_accuracy: 0.9869\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9849 - val_loss: 0.0540 - val_accuracy: 0.9838\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0502 - accuracy: 0.9851 - val_loss: 0.0514 - val_accuracy: 0.9846\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0515 - accuracy: 0.9838 - val_loss: 0.0489 - val_accuracy: 0.9854\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0509 - accuracy: 0.9846 - val_loss: 0.0537 - val_accuracy: 0.9846\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0520 - accuracy: 0.9856 - val_loss: 0.0512 - val_accuracy: 0.9846\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9856 - val_loss: 0.0503 - val_accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다. EarlyStopping callback \n",
    "# 학습이 진행되어도 테스트셋 오차가 줄어들지 않으면 학습을 자동으로 멈추게 하는 콜백 함수\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "#monitor 매개변수: model.fit()의 실행 결과 중 어느 것을 이용할지를 결정\n",
    "#patience 매개변수: 지정된 값이 몇 번 이상 향상되지 않을 때 학습을 종료시킬 지 결정\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "#save_best_only 옵션이 True라면 매번 모델을 저장하지 않고 모델의 성능이 개선될때만 모델을 저장함\n",
    "#즉 모니터링하는 지표(metric)이 이전까지의 최상의 값일 때에만 모델을 저장\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9846\n",
      "Test accuracy: 0.9846153855323792\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
