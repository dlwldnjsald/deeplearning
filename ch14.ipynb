{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 29ms/step - loss: 0.4194 - accuracy: 0.8073 - val_loss: 0.3073 - val_accuracy: 0.8731\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2984 - accuracy: 0.8920 - val_loss: 0.2650 - val_accuracy: 0.9008\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2743 - accuracy: 0.9027 - val_loss: 0.2564 - val_accuracy: 0.9008\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2617 - accuracy: 0.9040 - val_loss: 0.2388 - val_accuracy: 0.9015\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2508 - accuracy: 0.9097 - val_loss: 0.2318 - val_accuracy: 0.9038\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2436 - accuracy: 0.9104 - val_loss: 0.2247 - val_accuracy: 0.9100\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2381 - accuracy: 0.9138 - val_loss: 0.2193 - val_accuracy: 0.9131\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2331 - accuracy: 0.9143 - val_loss: 0.2138 - val_accuracy: 0.9154\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2303 - accuracy: 0.9153 - val_loss: 0.2098 - val_accuracy: 0.9185\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2263 - accuracy: 0.9207 - val_loss: 0.2066 - val_accuracy: 0.9185\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2229 - accuracy: 0.9199 - val_loss: 0.2036 - val_accuracy: 0.9215\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2207 - accuracy: 0.9199 - val_loss: 0.2004 - val_accuracy: 0.9246\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2188 - accuracy: 0.9238 - val_loss: 0.1992 - val_accuracy: 0.9246\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2157 - accuracy: 0.9228 - val_loss: 0.1953 - val_accuracy: 0.9292\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2143 - accuracy: 0.9253 - val_loss: 0.1949 - val_accuracy: 0.9292\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2126 - accuracy: 0.9217 - val_loss: 0.1909 - val_accuracy: 0.9308\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2109 - accuracy: 0.9279 - val_loss: 0.1896 - val_accuracy: 0.9292\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2092 - accuracy: 0.9276 - val_loss: 0.1880 - val_accuracy: 0.9292\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2068 - accuracy: 0.9284 - val_loss: 0.1860 - val_accuracy: 0.9346\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2052 - accuracy: 0.9284 - val_loss: 0.1836 - val_accuracy: 0.9354\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2037 - accuracy: 0.9294 - val_loss: 0.1853 - val_accuracy: 0.9346\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2021 - accuracy: 0.9294 - val_loss: 0.1790 - val_accuracy: 0.9369\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2003 - accuracy: 0.9281 - val_loss: 0.1785 - val_accuracy: 0.9385\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1967 - accuracy: 0.9310 - val_loss: 0.1757 - val_accuracy: 0.9408\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1940 - accuracy: 0.9325 - val_loss: 0.1815 - val_accuracy: 0.9346\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1932 - accuracy: 0.9310 - val_loss: 0.1704 - val_accuracy: 0.9415\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1887 - accuracy: 0.9325 - val_loss: 0.1718 - val_accuracy: 0.9369\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1869 - accuracy: 0.9330 - val_loss: 0.1657 - val_accuracy: 0.9462\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1827 - accuracy: 0.9376 - val_loss: 0.1666 - val_accuracy: 0.9438\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1814 - accuracy: 0.9394 - val_loss: 0.1608 - val_accuracy: 0.9469\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1793 - accuracy: 0.9394 - val_loss: 0.1599 - val_accuracy: 0.9477\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1787 - accuracy: 0.9407 - val_loss: 0.1547 - val_accuracy: 0.9485\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1702 - accuracy: 0.9423 - val_loss: 0.1555 - val_accuracy: 0.9492\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1672 - accuracy: 0.9433 - val_loss: 0.1489 - val_accuracy: 0.9500\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1658 - accuracy: 0.9435 - val_loss: 0.1501 - val_accuracy: 0.9515\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1619 - accuracy: 0.9459 - val_loss: 0.1464 - val_accuracy: 0.9508\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1600 - accuracy: 0.9469 - val_loss: 0.1422 - val_accuracy: 0.9538\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9466 - val_loss: 0.1422 - val_accuracy: 0.9562\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1545 - accuracy: 0.9487 - val_loss: 0.1449 - val_accuracy: 0.9546\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1551 - accuracy: 0.9487 - val_loss: 0.1376 - val_accuracy: 0.9577\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1529 - accuracy: 0.9482 - val_loss: 0.1375 - val_accuracy: 0.9569\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1482 - accuracy: 0.9479 - val_loss: 0.1399 - val_accuracy: 0.9577\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1475 - accuracy: 0.9489 - val_loss: 0.1335 - val_accuracy: 0.9577\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1476 - accuracy: 0.9510 - val_loss: 0.1347 - val_accuracy: 0.9592\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1435 - accuracy: 0.9520 - val_loss: 0.1284 - val_accuracy: 0.9600\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1395 - accuracy: 0.9528 - val_loss: 0.1272 - val_accuracy: 0.9592\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1384 - accuracy: 0.9546 - val_loss: 0.1212 - val_accuracy: 0.9592\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1368 - accuracy: 0.9541 - val_loss: 0.1204 - val_accuracy: 0.9623\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1332 - accuracy: 0.9561 - val_loss: 0.1187 - val_accuracy: 0.9615\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1356 - accuracy: 0.9551 - val_loss: 0.1192 - val_accuracy: 0.9592\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 2ms/step - loss: 0.1174 - accuracy: 0.9638\n",
      "Test accuracy: 0.9638461470603943\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all\\01-0.8531.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all\\02-0.8800.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all\\03-0.9254.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all\\04-0.9415.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all\\05-0.9408.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all\\06-0.9415.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all\\07-0.9431.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all\\08-0.9446.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all\\09-0.9423.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all\\10-0.9438.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all\\11-0.9454.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all\\12-0.9438.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all\\13-0.9438.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all\\14-0.9438.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all\\15-0.9438.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all\\16-0.9438.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all\\17-0.9438.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all\\18-0.9438.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all\\19-0.9423.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all\\20-0.9438.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all\\21-0.9431.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all\\22-0.9431.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all\\23-0.9431.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all\\24-0.9446.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all\\25-0.9431.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all\\26-0.9446.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all\\27-0.9438.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all\\28-0.9446.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all\\29-0.9431.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all\\30-0.9469.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all\\31-0.9454.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all\\32-0.9469.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all\\33-0.9462.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all\\34-0.9469.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all\\35-0.9469.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all\\36-0.9485.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all\\37-0.9500.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all\\38-0.9500.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all\\39-0.9500.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all\\40-0.9515.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all\\41-0.9500.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all\\42-0.9531.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all\\43-0.9538.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all\\44-0.9523.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all\\45-0.9546.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all\\46-0.9554.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all\\47-0.9554.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all\\48-0.9554.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all\\49-0.9569.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all\\50-0.9577.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 9ms/step - loss: 0.1563 - accuracy: 0.9400\n",
      "Test accuracy: 0.9399999976158142\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.143487</td>\n",
       "      <td>0.950475</td>\n",
       "      <td>0.132294</td>\n",
       "      <td>0.957692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137387</td>\n",
       "      <td>0.950731</td>\n",
       "      <td>0.131459</td>\n",
       "      <td>0.957692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.136631</td>\n",
       "      <td>0.951245</td>\n",
       "      <td>0.129593</td>\n",
       "      <td>0.957692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130822</td>\n",
       "      <td>0.953297</td>\n",
       "      <td>0.124548</td>\n",
       "      <td>0.956923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.127445</td>\n",
       "      <td>0.952271</td>\n",
       "      <td>0.122835</td>\n",
       "      <td>0.955385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.031929</td>\n",
       "      <td>0.990506</td>\n",
       "      <td>0.054197</td>\n",
       "      <td>0.986923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.029482</td>\n",
       "      <td>0.990249</td>\n",
       "      <td>0.054204</td>\n",
       "      <td>0.986154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.024971</td>\n",
       "      <td>0.992815</td>\n",
       "      <td>0.056489</td>\n",
       "      <td>0.988462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.025504</td>\n",
       "      <td>0.993328</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>0.988462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.992045</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>0.986154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.143487  0.950475  0.132294      0.957692\n",
       "1     0.137387  0.950731  0.131459      0.957692\n",
       "2     0.136631  0.951245  0.129593      0.957692\n",
       "3     0.130822  0.953297  0.124548      0.956923\n",
       "4     0.127445  0.952271  0.122835      0.955385\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.031929  0.990506  0.054197      0.986923\n",
       "1996  0.029482  0.990249  0.054204      0.986154\n",
       "1997  0.024971  0.992815  0.056489      0.988462\n",
       "1998  0.025504  0.993328  0.060988      0.988462\n",
       "1999  0.028942  0.992045  0.057174      0.986154\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 30)                390       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(12,)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 1s 28ms/step - loss: 2.0170 - accuracy: 0.7549 - val_loss: 1.6779 - val_accuracy: 0.7408\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.4556 - accuracy: 0.7549 - val_loss: 1.1644 - val_accuracy: 0.7408\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.9724 - accuracy: 0.7549 - val_loss: 0.7088 - val_accuracy: 0.7415\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.5427 - accuracy: 0.7693 - val_loss: 0.3601 - val_accuracy: 0.8054\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3632 - accuracy: 0.8601 - val_loss: 0.3935 - val_accuracy: 0.8946\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3420 - accuracy: 0.8802 - val_loss: 0.3135 - val_accuracy: 0.8485\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3169 - accuracy: 0.8530 - val_loss: 0.3053 - val_accuracy: 0.8577\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2958 - accuracy: 0.8737 - val_loss: 0.2779 - val_accuracy: 0.8885\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2789 - accuracy: 0.8997 - val_loss: 0.2632 - val_accuracy: 0.8985\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2605 - accuracy: 0.9071 - val_loss: 0.2458 - val_accuracy: 0.9008\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2431 - accuracy: 0.9112 - val_loss: 0.2277 - val_accuracy: 0.9146\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2253 - accuracy: 0.9217 - val_loss: 0.2109 - val_accuracy: 0.9246\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2144 - accuracy: 0.9289 - val_loss: 0.2016 - val_accuracy: 0.9292\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2098 - accuracy: 0.9307 - val_loss: 0.1968 - val_accuracy: 0.9292\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2072 - accuracy: 0.9325 - val_loss: 0.1945 - val_accuracy: 0.9277\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2053 - accuracy: 0.9328 - val_loss: 0.1931 - val_accuracy: 0.9315\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2037 - accuracy: 0.9330 - val_loss: 0.1915 - val_accuracy: 0.9323\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2022 - accuracy: 0.9333 - val_loss: 0.1895 - val_accuracy: 0.9292\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.2011 - accuracy: 0.9338 - val_loss: 0.1881 - val_accuracy: 0.9331\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1997 - accuracy: 0.9351 - val_loss: 0.1861 - val_accuracy: 0.9308\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1990 - accuracy: 0.9358 - val_loss: 0.1849 - val_accuracy: 0.9315\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1986 - accuracy: 0.9361 - val_loss: 0.1840 - val_accuracy: 0.9323\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1975 - accuracy: 0.9371 - val_loss: 0.1826 - val_accuracy: 0.9315\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1966 - accuracy: 0.9364 - val_loss: 0.1812 - val_accuracy: 0.9308\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1952 - accuracy: 0.9371 - val_loss: 0.1795 - val_accuracy: 0.9315\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1932 - accuracy: 0.9376 - val_loss: 0.1793 - val_accuracy: 0.9323\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1923 - accuracy: 0.9376 - val_loss: 0.1754 - val_accuracy: 0.9369\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1898 - accuracy: 0.9379 - val_loss: 0.1747 - val_accuracy: 0.9346\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1878 - accuracy: 0.9382 - val_loss: 0.1725 - val_accuracy: 0.9369\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1865 - accuracy: 0.9379 - val_loss: 0.1693 - val_accuracy: 0.9392\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1841 - accuracy: 0.9387 - val_loss: 0.1688 - val_accuracy: 0.9392\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1827 - accuracy: 0.9412 - val_loss: 0.1679 - val_accuracy: 0.9408\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1826 - accuracy: 0.9397 - val_loss: 0.1657 - val_accuracy: 0.9415\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.1791 - accuracy: 0.9400 - val_loss: 0.1641 - val_accuracy: 0.9423\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.1773 - accuracy: 0.9407 - val_loss: 0.1607 - val_accuracy: 0.9438\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1751 - accuracy: 0.9397 - val_loss: 0.1644 - val_accuracy: 0.9415\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.1740 - accuracy: 0.9430 - val_loss: 0.1561 - val_accuracy: 0.9423\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.1691 - accuracy: 0.9420 - val_loss: 0.1556 - val_accuracy: 0.9454\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.1673 - accuracy: 0.9423 - val_loss: 0.1526 - val_accuracy: 0.9431\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1660 - accuracy: 0.9425 - val_loss: 0.1527 - val_accuracy: 0.9469\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1652 - accuracy: 0.9423 - val_loss: 0.1483 - val_accuracy: 0.9492\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1616 - accuracy: 0.9433 - val_loss: 0.1467 - val_accuracy: 0.9477\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1597 - accuracy: 0.9418 - val_loss: 0.1461 - val_accuracy: 0.9469\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1591 - accuracy: 0.9443 - val_loss: 0.1418 - val_accuracy: 0.9492\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1556 - accuracy: 0.9438 - val_loss: 0.1394 - val_accuracy: 0.9508\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1534 - accuracy: 0.9448 - val_loss: 0.1378 - val_accuracy: 0.9515\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1517 - accuracy: 0.9453 - val_loss: 0.1377 - val_accuracy: 0.9523\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1521 - accuracy: 0.9466 - val_loss: 0.1347 - val_accuracy: 0.9500\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1487 - accuracy: 0.9466 - val_loss: 0.1327 - val_accuracy: 0.9492\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1470 - accuracy: 0.9474 - val_loss: 0.1309 - val_accuracy: 0.9538\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1471 - accuracy: 0.9461 - val_loss: 0.1293 - val_accuracy: 0.9538\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1433 - accuracy: 0.9484 - val_loss: 0.1262 - val_accuracy: 0.9523\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1422 - accuracy: 0.9484 - val_loss: 0.1259 - val_accuracy: 0.9569\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1394 - accuracy: 0.9497 - val_loss: 0.1206 - val_accuracy: 0.9592\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1355 - accuracy: 0.9528 - val_loss: 0.1261 - val_accuracy: 0.9585\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1326 - accuracy: 0.9525 - val_loss: 0.1202 - val_accuracy: 0.9585\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1303 - accuracy: 0.9548 - val_loss: 0.1163 - val_accuracy: 0.9600\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1273 - accuracy: 0.9556 - val_loss: 0.1117 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1253 - accuracy: 0.9561 - val_loss: 0.1109 - val_accuracy: 0.9615\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1241 - accuracy: 0.9564 - val_loss: 0.1096 - val_accuracy: 0.9615\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1235 - accuracy: 0.9574 - val_loss: 0.1086 - val_accuracy: 0.9615\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1219 - accuracy: 0.9569 - val_loss: 0.1085 - val_accuracy: 0.9638\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9566 - val_loss: 0.1079 - val_accuracy: 0.9592\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1198 - accuracy: 0.9577 - val_loss: 0.1050 - val_accuracy: 0.9600\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1235 - accuracy: 0.9574 - val_loss: 0.1048 - val_accuracy: 0.9631\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1254 - accuracy: 0.9577 - val_loss: 0.1044 - val_accuracy: 0.9615\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1147 - accuracy: 0.9602 - val_loss: 0.1099 - val_accuracy: 0.9600\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.1151 - accuracy: 0.9625 - val_loss: 0.1016 - val_accuracy: 0.9623\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1124 - accuracy: 0.9628 - val_loss: 0.1012 - val_accuracy: 0.9623\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1122 - accuracy: 0.9605 - val_loss: 0.0966 - val_accuracy: 0.9654\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1116 - accuracy: 0.9633 - val_loss: 0.0968 - val_accuracy: 0.9638\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1102 - accuracy: 0.9625 - val_loss: 0.1028 - val_accuracy: 0.9638\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1080 - accuracy: 0.9648 - val_loss: 0.0934 - val_accuracy: 0.9654\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1075 - accuracy: 0.9654 - val_loss: 0.0932 - val_accuracy: 0.9677\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1058 - accuracy: 0.9654 - val_loss: 0.0915 - val_accuracy: 0.9669\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1060 - accuracy: 0.9656 - val_loss: 0.0904 - val_accuracy: 0.9662\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1057 - accuracy: 0.9672 - val_loss: 0.0918 - val_accuracy: 0.9662\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1047 - accuracy: 0.9659 - val_loss: 0.0895 - val_accuracy: 0.9662\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1053 - accuracy: 0.9656 - val_loss: 0.0965 - val_accuracy: 0.9623\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1093 - accuracy: 0.9648 - val_loss: 0.0885 - val_accuracy: 0.9662\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1033 - accuracy: 0.9659 - val_loss: 0.0862 - val_accuracy: 0.9685\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1000 - accuracy: 0.9679 - val_loss: 0.0881 - val_accuracy: 0.9685\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0996 - accuracy: 0.9684 - val_loss: 0.0856 - val_accuracy: 0.9685\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0994 - accuracy: 0.9690 - val_loss: 0.0849 - val_accuracy: 0.9685\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0974 - accuracy: 0.9695 - val_loss: 0.0867 - val_accuracy: 0.9677\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1002 - accuracy: 0.9674 - val_loss: 0.0869 - val_accuracy: 0.9685\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1009 - accuracy: 0.9674 - val_loss: 0.1006 - val_accuracy: 0.9608\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9674 - val_loss: 0.1041 - val_accuracy: 0.9592\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1020 - accuracy: 0.9659 - val_loss: 0.0934 - val_accuracy: 0.9615\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1005 - accuracy: 0.9664 - val_loss: 0.0856 - val_accuracy: 0.9662\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0978 - accuracy: 0.9682 - val_loss: 0.0884 - val_accuracy: 0.9654\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0981 - accuracy: 0.9687 - val_loss: 0.0848 - val_accuracy: 0.9669\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0942 - accuracy: 0.9715 - val_loss: 0.0797 - val_accuracy: 0.9708\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0972 - accuracy: 0.9682 - val_loss: 0.0806 - val_accuracy: 0.9692\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0944 - accuracy: 0.9700 - val_loss: 0.0810 - val_accuracy: 0.9677\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0929 - accuracy: 0.9700 - val_loss: 0.0777 - val_accuracy: 0.9731\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0891 - accuracy: 0.9715 - val_loss: 0.0806 - val_accuracy: 0.9677\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0889 - accuracy: 0.9723 - val_loss: 0.0773 - val_accuracy: 0.9723\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0883 - accuracy: 0.9733 - val_loss: 0.0763 - val_accuracy: 0.9723\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0927 - accuracy: 0.9707 - val_loss: 0.0761 - val_accuracy: 0.9708\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0873 - accuracy: 0.9720 - val_loss: 0.0779 - val_accuracy: 0.9692\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0865 - accuracy: 0.9725 - val_loss: 0.0790 - val_accuracy: 0.9646\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0864 - accuracy: 0.9723 - val_loss: 0.0837 - val_accuracy: 0.9638\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0880 - accuracy: 0.9715 - val_loss: 0.0813 - val_accuracy: 0.9615\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0854 - accuracy: 0.9733 - val_loss: 0.0756 - val_accuracy: 0.9685\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0840 - accuracy: 0.9741 - val_loss: 0.0754 - val_accuracy: 0.9685\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0834 - accuracy: 0.9738 - val_loss: 0.0799 - val_accuracy: 0.9646\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0844 - accuracy: 0.9733 - val_loss: 0.0763 - val_accuracy: 0.9685\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0852 - accuracy: 0.9728 - val_loss: 0.0737 - val_accuracy: 0.9692\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0846 - accuracy: 0.9731 - val_loss: 0.0809 - val_accuracy: 0.9646\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0807 - accuracy: 0.9733 - val_loss: 0.0714 - val_accuracy: 0.9700\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0820 - accuracy: 0.9743 - val_loss: 0.0753 - val_accuracy: 0.9692\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0802 - accuracy: 0.9751 - val_loss: 0.0719 - val_accuracy: 0.9692\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0804 - accuracy: 0.9746 - val_loss: 0.0710 - val_accuracy: 0.9685\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0793 - accuracy: 0.9751 - val_loss: 0.0740 - val_accuracy: 0.9700\n",
      "Epoch 116/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0790 - accuracy: 0.9743 - val_loss: 0.0713 - val_accuracy: 0.9700\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0788 - accuracy: 0.9756 - val_loss: 0.0692 - val_accuracy: 0.9715\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0778 - accuracy: 0.9759 - val_loss: 0.0714 - val_accuracy: 0.9715\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0777 - accuracy: 0.9756 - val_loss: 0.0686 - val_accuracy: 0.9715\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0782 - accuracy: 0.9751 - val_loss: 0.0689 - val_accuracy: 0.9708\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0766 - accuracy: 0.9764 - val_loss: 0.0671 - val_accuracy: 0.9723\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0766 - accuracy: 0.9738 - val_loss: 0.0675 - val_accuracy: 0.9723\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0756 - accuracy: 0.9761 - val_loss: 0.0690 - val_accuracy: 0.9708\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0768 - accuracy: 0.9741 - val_loss: 0.0772 - val_accuracy: 0.9692\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9761 - val_loss: 0.0717 - val_accuracy: 0.9708\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0762 - accuracy: 0.9759 - val_loss: 0.0696 - val_accuracy: 0.9715\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0744 - accuracy: 0.9774 - val_loss: 0.0660 - val_accuracy: 0.9715\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0770 - accuracy: 0.9774 - val_loss: 0.0655 - val_accuracy: 0.9731\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0762 - accuracy: 0.9764 - val_loss: 0.0653 - val_accuracy: 0.9731\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0745 - accuracy: 0.9756 - val_loss: 0.0654 - val_accuracy: 0.9715\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0765 - accuracy: 0.9777 - val_loss: 0.0641 - val_accuracy: 0.9738\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0748 - accuracy: 0.9769 - val_loss: 0.0636 - val_accuracy: 0.9738\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0755 - accuracy: 0.9751 - val_loss: 0.0658 - val_accuracy: 0.9731\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0754 - accuracy: 0.9774 - val_loss: 0.0637 - val_accuracy: 0.9738\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0742 - accuracy: 0.9772 - val_loss: 0.0664 - val_accuracy: 0.9738\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0746 - accuracy: 0.9756 - val_loss: 0.0633 - val_accuracy: 0.9731\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0747 - accuracy: 0.9761 - val_loss: 0.0624 - val_accuracy: 0.9754\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0731 - accuracy: 0.9761 - val_loss: 0.0677 - val_accuracy: 0.9738\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0730 - accuracy: 0.9769 - val_loss: 0.0710 - val_accuracy: 0.9731\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0716 - accuracy: 0.9782 - val_loss: 0.0798 - val_accuracy: 0.9708\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0732 - accuracy: 0.9766 - val_loss: 0.0675 - val_accuracy: 0.9738\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0710 - accuracy: 0.9790 - val_loss: 0.0643 - val_accuracy: 0.9738\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0710 - accuracy: 0.9774 - val_loss: 0.0633 - val_accuracy: 0.9731\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0719 - accuracy: 0.9777 - val_loss: 0.0682 - val_accuracy: 0.9731\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0704 - accuracy: 0.9772 - val_loss: 0.0669 - val_accuracy: 0.9746\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0705 - accuracy: 0.9769 - val_loss: 0.0718 - val_accuracy: 0.9708\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9779 - val_loss: 0.0630 - val_accuracy: 0.9746\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0691 - accuracy: 0.9787 - val_loss: 0.0657 - val_accuracy: 0.9746\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0701 - accuracy: 0.9772 - val_loss: 0.0620 - val_accuracy: 0.9731\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0677 - accuracy: 0.9779 - val_loss: 0.0611 - val_accuracy: 0.9738\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 0.0600 - val_accuracy: 0.9746\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0698 - accuracy: 0.9779 - val_loss: 0.0603 - val_accuracy: 0.9738\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0693 - accuracy: 0.9766 - val_loss: 0.0593 - val_accuracy: 0.9738\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0682 - accuracy: 0.9795 - val_loss: 0.0620 - val_accuracy: 0.9746\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0667 - accuracy: 0.9782 - val_loss: 0.0644 - val_accuracy: 0.9754\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0677 - accuracy: 0.9795 - val_loss: 0.0599 - val_accuracy: 0.9738\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0675 - accuracy: 0.9800 - val_loss: 0.0607 - val_accuracy: 0.9762\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0676 - accuracy: 0.9792 - val_loss: 0.0618 - val_accuracy: 0.9731\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0674 - accuracy: 0.9792 - val_loss: 0.0584 - val_accuracy: 0.9738\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0673 - accuracy: 0.9777 - val_loss: 0.0587 - val_accuracy: 0.9762\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0649 - accuracy: 0.9810 - val_loss: 0.0581 - val_accuracy: 0.9754\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0656 - accuracy: 0.9782 - val_loss: 0.0580 - val_accuracy: 0.9754\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0655 - accuracy: 0.9808 - val_loss: 0.0601 - val_accuracy: 0.9746\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0679 - accuracy: 0.9779 - val_loss: 0.0574 - val_accuracy: 0.9762\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0653 - accuracy: 0.9790 - val_loss: 0.0657 - val_accuracy: 0.9746\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0679 - accuracy: 0.9790 - val_loss: 0.0590 - val_accuracy: 0.9746\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0680 - accuracy: 0.9810 - val_loss: 0.0653 - val_accuracy: 0.9731\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0669 - accuracy: 0.9784 - val_loss: 0.0593 - val_accuracy: 0.9777\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0644 - accuracy: 0.9787 - val_loss: 0.0661 - val_accuracy: 0.9738\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0664 - accuracy: 0.9805 - val_loss: 0.0662 - val_accuracy: 0.9723\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0670 - accuracy: 0.9790 - val_loss: 0.0790 - val_accuracy: 0.9692\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0688 - accuracy: 0.9774 - val_loss: 0.0745 - val_accuracy: 0.9715\n",
      "Epoch 173/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0706 - accuracy: 0.9774 - val_loss: 0.0778 - val_accuracy: 0.9700\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0795 - accuracy: 0.9743 - val_loss: 0.0585 - val_accuracy: 0.9769\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0710 - accuracy: 0.9795 - val_loss: 0.0555 - val_accuracy: 0.9777\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0670 - accuracy: 0.9795 - val_loss: 0.0574 - val_accuracy: 0.9769\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9805 - val_loss: 0.0595 - val_accuracy: 0.9777\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0633 - accuracy: 0.9797 - val_loss: 0.0584 - val_accuracy: 0.9777\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0626 - accuracy: 0.9795 - val_loss: 0.0573 - val_accuracy: 0.9769\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0624 - accuracy: 0.9808 - val_loss: 0.0557 - val_accuracy: 0.9746\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0629 - accuracy: 0.9800 - val_loss: 0.0562 - val_accuracy: 0.9777\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0626 - accuracy: 0.9813 - val_loss: 0.0568 - val_accuracy: 0.9754\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0628 - accuracy: 0.9808 - val_loss: 0.0570 - val_accuracy: 0.9777\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0630 - accuracy: 0.9802 - val_loss: 0.0593 - val_accuracy: 0.9762\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0621 - accuracy: 0.9802 - val_loss: 0.0591 - val_accuracy: 0.9762\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0615 - accuracy: 0.9818 - val_loss: 0.0583 - val_accuracy: 0.9777\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.0585 - val_accuracy: 0.9762\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0638 - accuracy: 0.9813 - val_loss: 0.0557 - val_accuracy: 0.9754\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0617 - accuracy: 0.9808 - val_loss: 0.0546 - val_accuracy: 0.9777\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0660 - accuracy: 0.9790 - val_loss: 0.0569 - val_accuracy: 0.9769\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0663 - accuracy: 0.9810 - val_loss: 0.0541 - val_accuracy: 0.9769\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0683 - accuracy: 0.9784 - val_loss: 0.0682 - val_accuracy: 0.9731\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0646 - accuracy: 0.9802 - val_loss: 0.0603 - val_accuracy: 0.9754\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0602 - accuracy: 0.9813 - val_loss: 0.0545 - val_accuracy: 0.9769\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0617 - accuracy: 0.9805 - val_loss: 0.0549 - val_accuracy: 0.9769\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0601 - accuracy: 0.9823 - val_loss: 0.0563 - val_accuracy: 0.9769\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9820 - val_loss: 0.0567 - val_accuracy: 0.9769\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9823 - val_loss: 0.0550 - val_accuracy: 0.9769\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9826 - val_loss: 0.0551 - val_accuracy: 0.9762\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0612 - accuracy: 0.9823 - val_loss: 0.0544 - val_accuracy: 0.9762\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0605 - accuracy: 0.9823 - val_loss: 0.0581 - val_accuracy: 0.9769\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0547 - val_accuracy: 0.9762\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0591 - accuracy: 0.9826 - val_loss: 0.0632 - val_accuracy: 0.9754\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9805 - val_loss: 0.0589 - val_accuracy: 0.9777\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0611 - accuracy: 0.9797 - val_loss: 0.0605 - val_accuracy: 0.9754\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0602 - accuracy: 0.9818 - val_loss: 0.0604 - val_accuracy: 0.9769\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9813 - val_loss: 0.0563 - val_accuracy: 0.9762\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0594 - accuracy: 0.9810 - val_loss: 0.0549 - val_accuracy: 0.9777\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0614 - accuracy: 0.9808 - val_loss: 0.0527 - val_accuracy: 0.9777\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0597 - accuracy: 0.9810 - val_loss: 0.0527 - val_accuracy: 0.9785\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0597 - accuracy: 0.9802 - val_loss: 0.0533 - val_accuracy: 0.9792\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0598 - accuracy: 0.9815 - val_loss: 0.0594 - val_accuracy: 0.9762\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9820 - val_loss: 0.0703 - val_accuracy: 0.9738\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0616 - accuracy: 0.9813 - val_loss: 0.0590 - val_accuracy: 0.9754\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0602 - accuracy: 0.9820 - val_loss: 0.0539 - val_accuracy: 0.9769\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0603 - accuracy: 0.9815 - val_loss: 0.0529 - val_accuracy: 0.9792\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0588 - accuracy: 0.9823 - val_loss: 0.0576 - val_accuracy: 0.9769\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9818 - val_loss: 0.0561 - val_accuracy: 0.9762\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0597 - accuracy: 0.9810 - val_loss: 0.0760 - val_accuracy: 0.9754\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9813 - val_loss: 0.0536 - val_accuracy: 0.9785\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0622 - accuracy: 0.9813 - val_loss: 0.0553 - val_accuracy: 0.9762\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 0.0527 - val_accuracy: 0.9785\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0596 - accuracy: 0.9813 - val_loss: 0.0511 - val_accuracy: 0.9785\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0590 - accuracy: 0.9823 - val_loss: 0.0516 - val_accuracy: 0.9785\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0585 - accuracy: 0.9823 - val_loss: 0.0519 - val_accuracy: 0.9792\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0576 - accuracy: 0.9820 - val_loss: 0.0515 - val_accuracy: 0.9792\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0576 - accuracy: 0.9820 - val_loss: 0.0510 - val_accuracy: 0.9777\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0574 - accuracy: 0.9820 - val_loss: 0.0550 - val_accuracy: 0.9777\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0584 - accuracy: 0.9813 - val_loss: 0.0592 - val_accuracy: 0.9785\n",
      "Epoch 230/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0585 - accuracy: 0.9828 - val_loss: 0.0530 - val_accuracy: 0.9777\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0578 - accuracy: 0.9826 - val_loss: 0.0524 - val_accuracy: 0.9785\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0513 - val_accuracy: 0.9792\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0521 - val_accuracy: 0.9792\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.0528 - val_accuracy: 0.9785\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0566 - accuracy: 0.9823 - val_loss: 0.0552 - val_accuracy: 0.9792\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0574 - accuracy: 0.9820 - val_loss: 0.0702 - val_accuracy: 0.9731\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0662 - accuracy: 0.9774 - val_loss: 0.0690 - val_accuracy: 0.9769\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0615 - accuracy: 0.9795 - val_loss: 0.0560 - val_accuracy: 0.9769\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0586 - accuracy: 0.9828 - val_loss: 0.0508 - val_accuracy: 0.9808\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0573 - accuracy: 0.9826 - val_loss: 0.0529 - val_accuracy: 0.9792\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0575 - accuracy: 0.9815 - val_loss: 0.0501 - val_accuracy: 0.9800\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0568 - accuracy: 0.9836 - val_loss: 0.0550 - val_accuracy: 0.9792\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0536 - val_accuracy: 0.9808\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0564 - accuracy: 0.9841 - val_loss: 0.0587 - val_accuracy: 0.9777\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0592 - accuracy: 0.9818 - val_loss: 0.0625 - val_accuracy: 0.9777\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0579 - accuracy: 0.9831 - val_loss: 0.0549 - val_accuracy: 0.9808\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0563 - accuracy: 0.9836 - val_loss: 0.0515 - val_accuracy: 0.9808\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0552 - accuracy: 0.9831 - val_loss: 0.0501 - val_accuracy: 0.9808\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0555 - accuracy: 0.9820 - val_loss: 0.0520 - val_accuracy: 0.9800\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0498 - val_accuracy: 0.9808\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0561 - accuracy: 0.9828 - val_loss: 0.0500 - val_accuracy: 0.9808\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0558 - accuracy: 0.9836 - val_loss: 0.0508 - val_accuracy: 0.9808\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0560 - accuracy: 0.9820 - val_loss: 0.0495 - val_accuracy: 0.9808\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0547 - accuracy: 0.9838 - val_loss: 0.0514 - val_accuracy: 0.9808\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0554 - accuracy: 0.9828 - val_loss: 0.0645 - val_accuracy: 0.9777\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.0599 - val_accuracy: 0.9777\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0589 - accuracy: 0.9815 - val_loss: 0.0634 - val_accuracy: 0.9769\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0582 - accuracy: 0.9820 - val_loss: 0.0513 - val_accuracy: 0.9815\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0568 - accuracy: 0.9831 - val_loss: 0.0516 - val_accuracy: 0.9808\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0566 - accuracy: 0.9846 - val_loss: 0.0499 - val_accuracy: 0.9800\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0548 - accuracy: 0.9836 - val_loss: 0.0511 - val_accuracy: 0.9808\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.0550 - val_accuracy: 0.9800\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0546 - accuracy: 0.9833 - val_loss: 0.0543 - val_accuracy: 0.9800\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0557 - accuracy: 0.9831 - val_loss: 0.0608 - val_accuracy: 0.9769\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 0.0517 - val_accuracy: 0.9808\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0574 - accuracy: 0.9813 - val_loss: 0.0488 - val_accuracy: 0.9800\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0600 - accuracy: 0.9815 - val_loss: 0.0499 - val_accuracy: 0.9808\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0556 - accuracy: 0.9820 - val_loss: 0.0498 - val_accuracy: 0.9815\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0597 - accuracy: 0.9810 - val_loss: 0.0485 - val_accuracy: 0.9808\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0606 - accuracy: 0.9800 - val_loss: 0.0539 - val_accuracy: 0.9800\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0572 - accuracy: 0.9846 - val_loss: 0.0493 - val_accuracy: 0.9792\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9836 - val_loss: 0.0493 - val_accuracy: 0.9800\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0536 - accuracy: 0.9838 - val_loss: 0.0538 - val_accuracy: 0.9815\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0549 - accuracy: 0.9820 - val_loss: 0.0495 - val_accuracy: 0.9823\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0543 - accuracy: 0.9851 - val_loss: 0.0502 - val_accuracy: 0.9815\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0571 - accuracy: 0.9831 - val_loss: 0.0526 - val_accuracy: 0.9792\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0574 - accuracy: 0.9823 - val_loss: 0.0530 - val_accuracy: 0.9792\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.0512 - val_accuracy: 0.9800\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0531 - accuracy: 0.9831 - val_loss: 0.0503 - val_accuracy: 0.9808\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0521 - accuracy: 0.9841 - val_loss: 0.0487 - val_accuracy: 0.9808\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0535 - accuracy: 0.9828 - val_loss: 0.0486 - val_accuracy: 0.9800\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0557 - accuracy: 0.9815 - val_loss: 0.0482 - val_accuracy: 0.9808\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0553 - accuracy: 0.9838 - val_loss: 0.0503 - val_accuracy: 0.9800\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0539 - accuracy: 0.9836 - val_loss: 0.0542 - val_accuracy: 0.9777\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0599 - accuracy: 0.9815 - val_loss: 0.0508 - val_accuracy: 0.9800\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0556 - accuracy: 0.9828 - val_loss: 0.0487 - val_accuracy: 0.9808\n",
      "Epoch 287/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0523 - accuracy: 0.9833 - val_loss: 0.0491 - val_accuracy: 0.9808\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0520 - accuracy: 0.9841 - val_loss: 0.0515 - val_accuracy: 0.9808\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 0.0497 - val_accuracy: 0.9815\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0525 - accuracy: 0.9846 - val_loss: 0.0489 - val_accuracy: 0.9815\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0522 - accuracy: 0.9846 - val_loss: 0.0486 - val_accuracy: 0.9815\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0525 - accuracy: 0.9828 - val_loss: 0.0508 - val_accuracy: 0.9800\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0521 - accuracy: 0.9838 - val_loss: 0.0493 - val_accuracy: 0.9808\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9808\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0525 - accuracy: 0.9851 - val_loss: 0.0476 - val_accuracy: 0.9808\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0523 - accuracy: 0.9851 - val_loss: 0.0475 - val_accuracy: 0.9815\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0550 - accuracy: 0.9828 - val_loss: 0.0525 - val_accuracy: 0.9792\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0547 - accuracy: 0.9851 - val_loss: 0.0477 - val_accuracy: 0.9800\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0527 - accuracy: 0.9849 - val_loss: 0.0480 - val_accuracy: 0.9815\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9841 - val_loss: 0.0510 - val_accuracy: 0.9808\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0529 - accuracy: 0.9838 - val_loss: 0.0506 - val_accuracy: 0.9815\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0527 - accuracy: 0.9828 - val_loss: 0.0473 - val_accuracy: 0.9808\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0484 - val_accuracy: 0.9815\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0515 - accuracy: 0.9859 - val_loss: 0.0519 - val_accuracy: 0.9769\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0523 - accuracy: 0.9849 - val_loss: 0.0476 - val_accuracy: 0.9823\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0511 - accuracy: 0.9851 - val_loss: 0.0471 - val_accuracy: 0.9815\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0509 - accuracy: 0.9841 - val_loss: 0.0494 - val_accuracy: 0.9823\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0507 - accuracy: 0.9841 - val_loss: 0.0478 - val_accuracy: 0.9815\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0512 - accuracy: 0.9846 - val_loss: 0.0568 - val_accuracy: 0.9800\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0690 - val_accuracy: 0.9746\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0574 - accuracy: 0.9815 - val_loss: 0.0574 - val_accuracy: 0.9800\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0573 - accuracy: 0.9823 - val_loss: 0.0486 - val_accuracy: 0.9815\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0533 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9800\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0557 - accuracy: 0.9823 - val_loss: 0.0544 - val_accuracy: 0.9777\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 0.0486 - val_accuracy: 0.9808\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0521 - accuracy: 0.9841 - val_loss: 0.0476 - val_accuracy: 0.9815\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9843 - val_loss: 0.0478 - val_accuracy: 0.9808\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0546 - accuracy: 0.9826 - val_loss: 0.0492 - val_accuracy: 0.9785\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0556 - accuracy: 0.9818 - val_loss: 0.0468 - val_accuracy: 0.9808\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0526 - accuracy: 0.9831 - val_loss: 0.0474 - val_accuracy: 0.9831\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0502 - accuracy: 0.9843 - val_loss: 0.0467 - val_accuracy: 0.9838\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0504 - accuracy: 0.9841 - val_loss: 0.0469 - val_accuracy: 0.9823\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0504 - accuracy: 0.9846 - val_loss: 0.0467 - val_accuracy: 0.9815\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0477 - val_accuracy: 0.9815\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0510 - accuracy: 0.9838 - val_loss: 0.0527 - val_accuracy: 0.9792\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0506 - accuracy: 0.9846 - val_loss: 0.0472 - val_accuracy: 0.9815\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0509 - accuracy: 0.9846 - val_loss: 0.0470 - val_accuracy: 0.9815\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0513 - accuracy: 0.9841 - val_loss: 0.0466 - val_accuracy: 0.9808\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0510 - accuracy: 0.9838 - val_loss: 0.0461 - val_accuracy: 0.9823\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9823 - val_loss: 0.0471 - val_accuracy: 0.9831\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0499 - accuracy: 0.9843 - val_loss: 0.0465 - val_accuracy: 0.9823\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0508 - accuracy: 0.9849 - val_loss: 0.0458 - val_accuracy: 0.9823\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0499 - accuracy: 0.9851 - val_loss: 0.0465 - val_accuracy: 0.9831\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9851 - val_loss: 0.0479 - val_accuracy: 0.9831\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0497 - accuracy: 0.9849 - val_loss: 0.0503 - val_accuracy: 0.9815\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0507 - accuracy: 0.9849 - val_loss: 0.0476 - val_accuracy: 0.9838\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0525 - accuracy: 0.9843 - val_loss: 0.0458 - val_accuracy: 0.9823\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9838 - val_loss: 0.0467 - val_accuracy: 0.9823\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0521 - accuracy: 0.9843 - val_loss: 0.0456 - val_accuracy: 0.9815\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0501 - accuracy: 0.9854 - val_loss: 0.0475 - val_accuracy: 0.9831\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0557 - accuracy: 0.9838 - val_loss: 0.0582 - val_accuracy: 0.9808\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.0553 - val_accuracy: 0.9792\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0507 - accuracy: 0.9859 - val_loss: 0.0456 - val_accuracy: 0.9831\n",
      "Epoch 344/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0493 - accuracy: 0.9867 - val_loss: 0.0483 - val_accuracy: 0.9823\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0495 - accuracy: 0.9854 - val_loss: 0.0516 - val_accuracy: 0.9808\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9849 - val_loss: 0.0518 - val_accuracy: 0.9815\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0521 - accuracy: 0.9851 - val_loss: 0.0554 - val_accuracy: 0.9792\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9851 - val_loss: 0.0467 - val_accuracy: 0.9823\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0495 - accuracy: 0.9841 - val_loss: 0.0455 - val_accuracy: 0.9815\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0503 - accuracy: 0.9859 - val_loss: 0.0456 - val_accuracy: 0.9823\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0503 - accuracy: 0.9846 - val_loss: 0.0457 - val_accuracy: 0.9823\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0454 - val_accuracy: 0.9815\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9843 - val_loss: 0.0450 - val_accuracy: 0.9831\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9838 - val_loss: 0.0525 - val_accuracy: 0.9792\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0539 - accuracy: 0.9805 - val_loss: 0.0497 - val_accuracy: 0.9823\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0503 - accuracy: 0.9851 - val_loss: 0.0453 - val_accuracy: 0.9838\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0491 - accuracy: 0.9864 - val_loss: 0.0464 - val_accuracy: 0.9823\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9846 - val_loss: 0.0492 - val_accuracy: 0.9831\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0481 - accuracy: 0.9856 - val_loss: 0.0450 - val_accuracy: 0.9831\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0480 - accuracy: 0.9864 - val_loss: 0.0447 - val_accuracy: 0.9831\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0493 - accuracy: 0.9843 - val_loss: 0.0475 - val_accuracy: 0.9831\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0478 - accuracy: 0.9864 - val_loss: 0.0522 - val_accuracy: 0.9792\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0601 - accuracy: 0.9802 - val_loss: 0.0606 - val_accuracy: 0.9769\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0603 - accuracy: 0.9805 - val_loss: 0.0517 - val_accuracy: 0.9800\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0614 - accuracy: 0.9802 - val_loss: 0.0450 - val_accuracy: 0.9823\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0549 - accuracy: 0.9828 - val_loss: 0.0453 - val_accuracy: 0.9838\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0692 - val_accuracy: 0.9746\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0535 - accuracy: 0.9831 - val_loss: 0.0505 - val_accuracy: 0.9823\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0492 - accuracy: 0.9856 - val_loss: 0.0470 - val_accuracy: 0.9823\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0482 - accuracy: 0.9864 - val_loss: 0.0477 - val_accuracy: 0.9838\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0491 - accuracy: 0.9843 - val_loss: 0.0476 - val_accuracy: 0.9831\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9854 - val_loss: 0.0447 - val_accuracy: 0.9823\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0471 - accuracy: 0.9874 - val_loss: 0.0451 - val_accuracy: 0.9823\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9861 - val_loss: 0.0456 - val_accuracy: 0.9838\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9846 - val_loss: 0.0457 - val_accuracy: 0.9815\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9851 - val_loss: 0.0483 - val_accuracy: 0.9838\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0490 - accuracy: 0.9864 - val_loss: 0.0474 - val_accuracy: 0.9838\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0479 - accuracy: 0.9861 - val_loss: 0.0445 - val_accuracy: 0.9823\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9867 - val_loss: 0.0484 - val_accuracy: 0.9800\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0492 - accuracy: 0.9846 - val_loss: 0.0451 - val_accuracy: 0.9838\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9856 - val_loss: 0.0445 - val_accuracy: 0.9831\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0481 - accuracy: 0.9869 - val_loss: 0.0447 - val_accuracy: 0.9831\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9846 - val_loss: 0.0446 - val_accuracy: 0.9831\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9861 - val_loss: 0.0449 - val_accuracy: 0.9815\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0496 - accuracy: 0.9851 - val_loss: 0.0473 - val_accuracy: 0.9808\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0494 - accuracy: 0.9841 - val_loss: 0.0446 - val_accuracy: 0.9815\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0480 - accuracy: 0.9849 - val_loss: 0.0472 - val_accuracy: 0.9823\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0502 - accuracy: 0.9838 - val_loss: 0.0495 - val_accuracy: 0.9831\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9849 - val_loss: 0.0493 - val_accuracy: 0.9815\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0483 - accuracy: 0.9851 - val_loss: 0.0460 - val_accuracy: 0.9823\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0474 - accuracy: 0.9856 - val_loss: 0.0532 - val_accuracy: 0.9815\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0490 - accuracy: 0.9849 - val_loss: 0.0508 - val_accuracy: 0.9823\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 0.0448 - val_accuracy: 0.9846\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9872 - val_loss: 0.0479 - val_accuracy: 0.9785\n",
      "Epoch 395/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9831 - val_loss: 0.0450 - val_accuracy: 0.9815\n",
      "Epoch 396/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0486 - accuracy: 0.9846 - val_loss: 0.0470 - val_accuracy: 0.9808\n",
      "Epoch 397/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0509 - accuracy: 0.9836 - val_loss: 0.0512 - val_accuracy: 0.9792\n",
      "Epoch 398/2000\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0453 - val_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다. EarlyStopping callback \n",
    "# 학습이 진행되어도 테스트셋 오차가 줄어들지 않으면 학습을 자동으로 멈추게 하는 콜백 함수\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "#monitor 매개변수: model.fit()의 실행 결과 중 어느 것을 이용할지를 결정\n",
    "#patience 매개변수: 지정된 값이 몇 번 이상 향상되지 않을 때 학습을 종료시킬 지 결정\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "#save_best_only 옵션이 True라면 매번 모델을 저장하지 않고 모델의 성능이 개선될때만 모델을 저장함\n",
    "#즉 모니터링하는 지표(metric)이 이전까지의 최상의 값일 때에만 모델을 저장\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9838\n",
      "Test accuracy: 0.983846127986908\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
